{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVmT47LOBy6i"
   },
   "source": [
    " <center> <h1> <b> Pattern Recognition and Machine Learning (EE5607) </b> </h1> </center>\n",
    "\n",
    "<b> Programming Assignment - 01 </b>\n",
    "\n",
    "<b> Instructions </b>\n",
    "1. Plagiarism is strictly prohibited.\n",
    "2. Delayed submissions will be penalized with a scaling factor of 0.5 per day.\n",
    "3. Please DO NOT use any machine learning libraries unless and otherwise specified.\n",
    "\n",
    "NOTE: First 4 questions are taken from: http://lcsl.mit.edu/courses/cbmmss/machine_learning/labs/Lab1.html\n",
    "\n",
    "\n",
    "<b> Assignment on K-nearest neighbour </b>\n",
    "\n",
    "<b> Part - 1 :  Data Generation </b>\n",
    "1. Use Gaussian distribution with appropriate parameters and produce a dataset with four classes and 30 samples per class: the classes must live in the 2D space and be centered on the corners of the unit square (0,0), (0,1) (1,1), (1,0), all with variance 0.3.\n",
    "2. Obtain a 2-class train set [X, Y] by having data on opposite corners sharing the same class with labels +1 and -1.\n",
    "3. Generate a test set [Xte, Yte] from the same distribution, starting with 200 samples per class.\n",
    "4. Visualize both sets using scatter plot on a 2-D plane.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Exk1ALx4LXWA"
   },
   "outputs": [],
   "source": [
    "#All imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjgkoGQOEjis",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "#Part1-(1) #Generating training data\n",
    "########################################\n",
    "#Define means and covariances \n",
    "mean1= np.array([0, 0])\n",
    "mean2= np.array([0, 1])\n",
    "mean3= np.array([1, 1])\n",
    "mean4= np.array([1, 0])\n",
    "cov= np.matrix([[0.3, 0], [0, 0.3]])\n",
    "#You may use \"np.random.multivariate_normal\" function \n",
    "A_train = np.random.multivariate_normal(mean=mean1, cov=cov, size=(30))\n",
    "B_train = np.random.multivariate_normal(mean=mean2, cov=cov, size=(30))\n",
    "C_train = np.random.multivariate_normal(mean=mean3, cov=cov, size=(30))\n",
    "D_train = np.random.multivariate_normal(mean=mean4, cov=cov, size=(30))\n",
    "\n",
    "\n",
    "########################################\n",
    "#Part1-(b) #Generating training labels\n",
    "########################################\n",
    "#  Point  -> Class\n",
    "# ----------------\n",
    "# A_train -> +1\n",
    "# D_train -> +1\n",
    "# B_train -> -1\n",
    "# C_train -> -1\n",
    "ones_train = np.ones((60, 1), dtype=int)\n",
    "X_train = np.vstack((A_train, C_train, B_train, D_train))\n",
    "Y_train = np.vstack((ones_train, -1*ones_train))\n",
    "\n",
    "def shuffle_dataset(X, Y):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    shuffler = np.arange(m)\n",
    "    np.random.shuffle(shuffler)\n",
    "    \n",
    "    X = X[shuffler, :]\n",
    "    Y = Y[shuffler, :]\n",
    "\n",
    "shuffle_dataset(X_train, Y_train)\n",
    "\n",
    "print(\"Size of X_train := {}, Y_train := {}\".format(X_train.shape, Y_train.shape))\n",
    "\n",
    "########################################\n",
    "#Part1-(c) #Generating testing data with labels\n",
    "########################################\n",
    "A_test = np.random.multivariate_normal(mean=mean1, cov=cov, size=(200))\n",
    "B_test = np.random.multivariate_normal(mean=mean2, cov=cov, size=(200))\n",
    "C_test = np.random.multivariate_normal(mean=mean3, cov=cov, size=(200))\n",
    "D_test = np.random.multivariate_normal(mean=mean4, cov=cov, size=(200))\n",
    "\n",
    "ones_test = np.ones((400, 1), dtype=int)\n",
    "X_test = np.vstack((A_test, C_test, B_test,  D_test))\n",
    "Y_test = np.vstack((ones_test, -1*ones_test))\n",
    "\n",
    "shuffle_dataset(X_test, Y_test)\n",
    "\n",
    "print(\"Size of X_test := {}, Y_test := {}\".format(X_test.shape, Y_test.shape))\n",
    "\n",
    "########################################\n",
    "#Part1-(d) #Visualizing data\n",
    "########################################\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "fig, ax = plt.subplots()\n",
    "ax.title.set_text(\"Data\")\n",
    "scatter = ax.scatter(X_test[:, 0], X_test[:, 1], c=Y_test, vmin=-3, vmax=3, cmap=\"Spectral\")\n",
    "ax.plot(mean1[0], mean1[1], 'r*', markersize=8)\n",
    "ax.plot(mean2[0], mean2[1], 'r*', markersize=8)\n",
    "ax.plot(mean3[0], mean3[1], 'r*', markersize=8)\n",
    "ax.plot(mean4[0], mean4[1], 'r*', markersize=8)\n",
    "ax.grid()\n",
    "ax.legend(*scatter.legend_elements(), title=\"Label\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTeUZYn2vy0H"
   },
   "source": [
    "<b> Part - 2: K-NN Classification  </b> The k-Nearest Neighbors algorithm (kNN) assigns to a test point the most frequent label of its k closest examples in the training set.\n",
    "1. Write a function kNNClassify to generate predictions Yp for the 2-class data generated at Part1. Pick a ”reasonable” k.\n",
    "2. Evaluate the classification performance (prediction error) by comparing the predicted labels Y_pred to the true labels Y_test.\n",
    "3. Visualize the obtained results, e.g. by plotting the wrongly classified points using different colors/markers\n",
    "4. Write a function to generate & visualize the decision regions of the 2D plane that are associated with each class, for a given classifier. Overlay the test points using scatter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJ6Ae731HkUP",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "#Part2-(1) #Define kNNClassify Function\n",
    "########################################\n",
    "def distance_function(x1, x2):\n",
    "    return np.linalg.norm(x1-x2, axis=1)\n",
    "\n",
    "def kNNClassify(X_train, Y_train, X_test, k):\n",
    "    Yp = []\n",
    "    \n",
    "    for idx in range(X_test.shape[0]):\n",
    "        x_test = X_test[idx, :]\n",
    "        sorter = np.argsort(distance_function(X_train, x_test))\n",
    "        Y_sorted = Y_train[sorter][:k, 0]\n",
    "        (unique, count) = np.unique(Y_sorted, return_counts=True)\n",
    "        Yp.append(unique[np.argmax(count)])\n",
    "    return np.array(Yp).reshape(-1, 1)\n",
    "\n",
    "k = 1\n",
    "Yp = kNNClassify(X_train, Y_train, X_test, k)\n",
    "\n",
    "\n",
    "########################################\n",
    "#Part2-(2) #Define a function to evaluate the performance\n",
    "########################################\n",
    "def KNNAccuracy(true, pred):\n",
    "    return np.mean(true==pred)\n",
    "print(\"Accuracy of kNN classifier with k as {} is {}%\".format(k, np.round(KNNAccuracy(Y_test, Yp)*100), 2))\n",
    "\n",
    "#Compute and print the classification accuracy \n",
    "\n",
    "\n",
    "########################################\n",
    "#Part2-(c) #Plot the error points with different color\n",
    "########################################\n",
    "fig, ax = plt.subplots()\n",
    "ax.title.set_text(\"Data\")\n",
    "scatter = ax.scatter(X_test[:, 0], X_test[:, 1], c=(Y_test==Yp), vmin=-3, vmax=3, cmap=\"coolwarm\")\n",
    "ax.plot(mean1[0], mean1[1], 'r*', markersize=8)\n",
    "ax.plot(mean2[0], mean2[1], 'r*', markersize=8)\n",
    "ax.plot(mean3[0], mean3[1], 'r*', markersize=8)\n",
    "ax.plot(mean4[0], mean4[1], 'r*', markersize=8)\n",
    "ax.grid()\n",
    "ax.legend(handles=scatter.legend_elements()[0], labels=[\"False\", \"True\"], title=\"Predictions\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UP_gGkTIAupD"
   },
   "source": [
    "<b> Part - 3: Parameter selection: What is good value for k?  </b>\n",
    "\n",
    "So far we considered an arbitrary choice for k. You will now wrrite the function *holdoutCVkNN* for model selection\n",
    "1. Perform hold-out cross-validation by setting aside a fraction (ρ of the training set for\n",
    "validation. Note: You may use ρ = 0.3, and repeat the procedure 10 times. The hold-out\n",
    "procedure may be quite unstable.\n",
    "\n",
    "\n",
    "> (a). Use a large range of candidate values for k (e.g. k = 1, 3, 5..., 21). Notice odd numbers are considered to avoid ties.\n",
    "\n",
    "> (b). Repeat the process for 10 times using a random cross-validation set each time with\n",
    "a ρ = 0.3\n",
    "\n",
    "> (c). Plot the training and validation errors for the different values of k.\n",
    "\n",
    "> (d). How would you now answer the question \"what is the best value for k\"?\n",
    "\n",
    "2. How is the value of k affected by ρ (percentage of points held out) and number of\n",
    "repetitions? What does a large number of repetitions provide?\n",
    "3. Apply the model obtained by cross-validation (i.e., best k) to the test set and check\n",
    "if there is an improvement on the classification error over the result of Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "By2YRUCLwSe7"
   },
   "outputs": [],
   "source": [
    "####################################\n",
    "#Part - 3 (1)\n",
    "####################################\n",
    "\n",
    "def split_dataset(X, Y, validation_split):\n",
    "    m = X.shape[0]\n",
    "    m_valid = int(validation_split*m)\n",
    "    \n",
    "    shuffler = np.arange(m)\n",
    "    np.random.shuffle(shuffler)\n",
    "\n",
    "    X_train = X[shuffler[:(m-m_valid)]]\n",
    "    Y_train = Y[shuffler[:(m-m_valid)]]\n",
    "    X_valid = X[shuffler[-1*(m_valid):]]\n",
    "    Y_valid = Y[shuffler[-1*(m_valid):]]\n",
    "    \n",
    "    return X_train, X_valid, Y_train, Y_valid \n",
    "    \n",
    "#Define holdoutCVkNN() Function\n",
    "def holdoutCVkNN(X, Y, k_range, numrep, rho):\n",
    "    \n",
    "    training_errors = np.zeros((len(k_range), numrep))\n",
    "    validation_errors = np.empty_like(training_errors)\n",
    "    \n",
    "    for idx, k in enumerate(k_range):\n",
    "        for i in range(numrep):\n",
    "            X_train, X_valid, Y_train, Y_valid = split_dataset(X, Y, rho)\n",
    "            \n",
    "            Y_train_pred = kNNClassify(X_train, Y_train, X_train, k)\n",
    "            Y_valid_pred = kNNClassify(X_train, Y_train, X_valid, k)\n",
    "            \n",
    "            training_errors[idx, i] = KNNAccuracy(Y_train, Y_train_pred)\n",
    "            validation_errors[idx, i] = KNNAccuracy(Y_valid, Y_valid_pred)\n",
    "    return training_errors, validation_errors\n",
    "\n",
    "X = np.vstack((X_train, X_test))\n",
    "Y = np.vstack((Y_train, Y_test))\n",
    "\n",
    "k_range = np.arange(1, 23, 2)\n",
    "rho = 0.3\n",
    "numrep = 10\n",
    "\n",
    "training_errors, validation_errors = holdoutCVkNN(X, Y, k_range, numrep, rho)\n",
    "\n",
    "#Plot training and validation errors for different values of k \n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.title(\"Accuracy vs k\")\n",
    "plt.plot(k_range, np.mean(training_errors, axis=1)*100, \"--ro\", label=\"Training error\")\n",
    "plt.plot(k_range, np.mean(validation_errors, axis=1)*100, \"--go\", label=\"Validation error\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.ylabel(\"Accuracy %\")\n",
    "\n",
    "\n",
    "#what is the best value for k? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################\n",
    "#Part3 - (2): Effect of rho and number repetations on k\n",
    "####################################\n",
    "training_errors_more_itrs_0_1, validation_errors_more_itrs_0_1 = holdoutCVkNN(X, Y, k_range, 200, 0.1)\n",
    "training_errors_more_itrs_0_2, validation_errors_more_itrs_0_2 = holdoutCVkNN(X, Y, k_range, 200, 0.2)\n",
    "training_errors_more_itrs_0_3, validation_errors_more_itrs_0_3 = holdoutCVkNN(X, Y, k_range, 200, 0.3)\n",
    "training_errors_more_itrs_0_4, validation_errors_more_itrs_0_4 = holdoutCVkNN(X, Y, k_range, 200, 0.4)\n",
    "training_errors_more_itrs_0_5, validation_errors_more_itrs_0_5 = holdoutCVkNN(X, Y, k_range, 200, 0.5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################\n",
    "#Part3 - (3): Evaluate the performance on test set with the best hyper parameters ( i.e best k ). \n",
    "####################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2PePhkvBCMA"
   },
   "source": [
    "<b> Part - 4: Size of training data and kNN regression  </b>\n",
    "\n",
    "1. Dependence on training size: Evaluate the performance as the size of the training set\n",
    "grows, e.g., n = {50, 100, 300, 500,...}. How would you choose a good range for k as n\n",
    "changes? What can you say about the stability of the solution? Check by repeating the\n",
    "validation multiple times.\n",
    "2. Try classifying more difficult datasets, for instance, by increasing the variance or\n",
    "adding noise by randomly flipping the labels on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bmCDoFLD05nH"
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "#Part4 - (1): Performance evaluation as n increases\n",
    "##################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################################\n",
    "#Part4 - (2): Ablation analysis : Mention classification accuracy on dataset with changing the variance and noise level\n",
    "##################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQ9W7st9BOVA"
   },
   "source": [
    "<b> Part - 5: Digit classification on MNIST data  </b>\n",
    "[[link to download MNIST data]( http://yann.lecun.com/exdb/mnist/)]\n",
    "1. Modify the function kNNClassify to handle multi-class problems and hence design a KNN classifier to classify the images in MNIST dataset as one of the 10 digits. The 28x28 images may be flattened to arrive at a 784 dimensional vector.\n",
    "> NOTE : If you had already written a kNNClassify for multi class classification in part 2, you are free to use it.\n",
    "2. Empirically determine the most suitable error function, and the corresponding k to maximize the performance on the cross-validation experiments.\n",
    "3. Apply these values to evaluate the performance on the test dataset.\n",
    "4. Create a confusion matrix to understand the most confused classes (digits).\n",
    "5. Suggest alternate ways to improve the performance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3vVynRcFZ5Kk"
   },
   "outputs": [],
   "source": [
    "#Mount the drive\n",
    "from google.colab import drive\n",
    "from mlxtend.data import loadlocal_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# MNIST Data reading ( you may define your own function for reading MNIST data )\n",
    "#Extract .gz files and place in the drive \n",
    "drive.mount('/content/drive',force_remount=True)\n",
    "root_path='/content/drive/MyDrive/PRMLAssignments/ProgrammingAssignment-01'\n",
    "train_data_path = str(root_path) + \"/\" + 'train-images-idx3-ubyte'\n",
    "train_label_path = str(root_path) + \"/\" + 'train-labels-idx1-ubyte'\n",
    "test_data_path = str(root_path) + \"/\" + 't10k-images-idx3-ubyte'\n",
    "test_label_path = str(root_path) + \"/\" + 't10k-labels-idx1-ubyte'\n",
    "X_train, Y_train = loadlocal_mnist(images_path=train_data_path, labels_path=train_label_path)\n",
    "X_test, Y_test = loadlocal_mnist(images_path=test_data_path, labels_path=test_label_path)\n",
    "\n",
    "def MultiClassKNNClassify(X_train,Y_train,X_test,k,error_func):\n",
    "  #error_func : For empirical selection of error function. egs : euclidean or any other distance metrics\n",
    "  #Initialize an array/list to store the labels\n",
    "  #Iterate through the test data  \n",
    "    #Iterate through the training data\n",
    "      #Compute the error between test data and training data  \n",
    "    #Obtain k closest training data points\n",
    "    #Assign label to test point based on majority voting\n",
    "  #return Y_pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################\n",
    "#Part5-(2) Empirically chose most suitable k and error function based on the evauation on cross-validation data\n",
    "########################################\n",
    "#You may use fraction of training data for validation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################\n",
    "#Part5-(3) Evaluate the performance on test data with the best hyper parameters ( k, error_func ) obtained from cross validation\n",
    "########################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################\n",
    "#Part5-(4) Create a confusion matrix for test data\n",
    "########################################\n",
    "#Define a function to obain the confusion matrix table\n",
    "def compute_confusion_matrix(true, pred):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################\n",
    "#Part5-(5) Suggest an alternative ways to improve performance\n",
    "########################################\n",
    "1.\n",
    "2.\n",
    "3.\n",
    "4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OohdgUOoAenj"
   },
   "source": [
    "<b> Report  </b>\n",
    "1. Write down the best accuracy on synthetic test data generated from Gaussian distribution\n",
    "\n",
    "2. Write down the best accuracy on MNIST validation and test data.\n",
    "\n",
    "3. Report your analysis\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PRML_Assignment01.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
