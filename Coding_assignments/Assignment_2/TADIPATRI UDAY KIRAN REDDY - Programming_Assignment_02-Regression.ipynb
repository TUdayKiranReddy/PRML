{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PRML_Assignment02.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVmT47LOBy6i"
      },
      "source": [
        " <center> <h1> <b> Pattern Recognition and Machine Learning (EE5607) </b> </h1> </center>\n",
        "\n",
        "<b> Programming Assignment - 02 </b>\n",
        "\n",
        "<b> Instructions </b>\n",
        "1. Plagiarism is strictly prohibited.\n",
        "2. Delayed submissions will be penalized with a scaling factor of 0.5 per day.\n",
        "3. Please DO NOT use any machine learning libraries unless and otherwise specified.\n",
        "\n",
        "\n",
        "\n",
        "<b> Assignment on Linear Regression </b> <br>\n",
        "This programing assignment is divided in to two parts covering the understanding of basic parts where you will get a chance to code the linear regression problem in both data space and kernel space. You will also implement regularizers to understand the bias-variance trade-off problem. In contrast to the full batch training, you will also get chance to code the online training of linear regression problem as discussed in the class. Part1 will conclude with the implimentation of MAP estimate. Later, In part2 of the assignment you have to apply the learned basic concepts to real world problems. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center> <h2> <b> Understanding Basic Concepts </b> </h2> </center>\n",
        "\n",
        "\n",
        "<b> Part - (1) :  Understanding Error Surfaces </b>\n",
        "\n",
        "According to www.geogebra.org, the relationship between human height (in\n",
        "inches) and weight (in pounds) is given by <br>\n",
        "<center> $t = 3.86x - 110.42$ </center>\n",
        "\n",
        "(a) Generate 25 meaningful data points from this relationship, mimicking a\n",
        "noisy sensor, where the noise follows a zero mean Gaussian with a variance\n",
        "of 20. Plot the scatter plot of the data. <br>\n",
        "(b) Now, we need to estimate the above relationship from the noisy data\n",
        "generated in (a) by fitting a line, i.e $\\hat{t} = y(x,w) = w_{0} + w_{1}x$. Let us use least squares criterion discussed in the class to estimate the parameters $w_{0}$ and $w_{1}$ Generate and plot the error surface $J(w_{0},w_{1})$ associated with this approach. Locate the minimum on this error surface.<br>\n",
        "\n",
        "(c) Estimate the parameters using least squares approach, and compare them\n",
        "with the desired values.\n",
        "<center> $\\textbf{w}_{opt} = (\\textbf{X}^{T}\\textbf{X})^{-1}\\textbf{X}^{T}\\textbf{t}$</center>\n",
        "\n",
        "(d) Report all your observations"
      ],
      "metadata": {
        "id": "etb5RPr5_qAw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Exk1ALx4LXWA"
      },
      "source": [
        "#All imports\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "import collections"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "#Generate meaningfull data \n",
        "########################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Plot scatter plot of data\n",
        "########################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Parameter prediction by locating the minima of error surface\n",
        "########################################\n",
        "#Complete the below error function \n",
        "def Error(w,t,x): #inputs : 1)weight 2)data i.e (t,x)\n",
        "\n",
        "\n",
        "\n",
        "    return error\n",
        "\n",
        "#Sample a bunch of w's around w_opt and compute the associated error\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Plot 3D error surface and the corresponding contour plots\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Locate the minima of the error surface\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Least squares approach to estimate the weights\n",
        "########################################\n",
        "#Complete the below linear regression function\n",
        "def LinearRegression(x,t) #inputs : 1)input data i.e (x). 2)target i.e (t)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return w_opt\n",
        "\n",
        "#Estimate optimal weight's using \"LinearRegression\" function\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Estimate the targets using the input x and the estimated weights\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Plot the estimated line on top of the above scatter plot\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Compare the estimated weight's using least squares approach with the error surface approach\n",
        "########################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DjgkoGQOEjis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTeUZYn2vy0H"
      },
      "source": [
        "<b> Part - (2) : Understanding model order and overfitting  </b>\n",
        "\n",
        "(a). Generate  20  data  points  from $t_{n} = sin(2πx_{n}) + e_{n}$, where $x_{n} \\in [0,1] $ and $e_{n} \\thicksim \\mathcal{N} (0,0.1)$ , and divide them into two sets, a training set and a testing set each containing 10 points <br>\n",
        "\n",
        "(b). Fit  an $M^{th}$ degree  polynomial  to  the  training  data  using  least  squares approach, i.e.,\n",
        "<center> $\\hat{t_{n}} = w_{0}x + w_{1}x + .... +  w_{m}x^{m} + ... + w_{M}x^{M} $ </center> \n",
        "\n",
        "Use the estimated parameter vector $\\textbf{w}$ , to predict the target values in training and testing datasets.  Plot the root mean squared error associated with each dataset, for M=0,1,...,9. Explain your results. <br>\n",
        "\n",
        "(c) Increase the size of the training dataset to 100 points, and repeat (b). <br>\n",
        "\n",
        "(d) Add a $l_{2}$ regularization term to the objective function in (b) and repeat (b) and (c).  Study the affect of Lagrange multiplier λ on the root mean squared error of the training and testing datasets <br>\n",
        "\n",
        "(e) Modify the function in (a) to $t_{n}=5+sin(2πx_{n})+e_{n}$ to study the effect of regularizing the bias coefficient $w_{0}$.\n",
        "\n",
        "(f) Report all your observations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "#Generate 20 data points\n",
        "########################################\n",
        "\n",
        "\n",
        "\n",
        "#Obtain train and test splits\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Fitting Mth degree polynomial using least squares approach\n",
        "########################################\n",
        "#Complete the function\n",
        "def PolynomialFit(X_train,Y_train,M,lamda):\n",
        "\n",
        "\n",
        "\n",
        "    return w_opt\n",
        "#Complete the function\n",
        "def PolynomialPred(w_est,X_train,X_test,Y_train,Y_test):\n",
        "\n",
        "\n",
        "\n",
        "    return TrainError,TestError\n",
        "#Iterate through range of M values\n",
        "M_range=list(range(10))\n",
        "for M in M_range:\n",
        "    #Fit Mth order polynomial i.e estimate optimal w\n",
        "\n",
        "\n",
        "\n",
        "    #Predict errors on both training and testing data using estimated w\n",
        "\n",
        "\n",
        "\n",
        "    #Store them for plotting\n",
        "\n",
        "#Plot training error vs polynomial order and testing error vs polynomial order\n",
        "\n",
        "########################################\n",
        "#Increase the size of training data set to 100 points and repeat the experiments\n",
        "########################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Effect of regularization\n",
        "########################################\n",
        "#Consider a set of lambda's\n",
        "\n",
        "#Repeat (b) and (c). Submit the plots for M=3,5,7\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Effect of bias regularization\n",
        "########################################\n",
        "#Modify the function i.e include bias\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Generate data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Estimate the polynomial with and without regularization constraint\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Compare the two estimated polynomials and report the observations\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JJ6Ae731HkUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Part - (3) : Understanding choice of kernel  </b>\n",
        "\n",
        "\n",
        "(a). Generate 100 data points from $t_{n}=sin(2πx_{n})+e_{n}$, where $x_{n} \\in [0 1]$ and $e_{n} \\thicksim \\mathcal{N}(0,0.1)$, and divide them into two sets, a training set and a testing test each containing 50 points.  Fit an $M^{th}$ degree polynomial using polynomial,Gaussian and sigmoidal kernels, and study the goodness of fit in each case,for different model orders M\n",
        "\n",
        "(b). Repeat (a) by modifying the target function to <br>\n",
        "<center> $t_{n} = $ $\\begin{cases}\n",
        " \\text{sinusoid} + e_{n} , \\;\\; where \\;\\; x  \\in [0,1) \\\\\n",
        " \\text{triangle} + e_{n} , \\;\\; where \\;\\; x  \\in [1,2) \\\\\n",
        " \\text{Gaussian} + e_{n} , \\;\\; where \\;\\; x  \\in [2,3) \\\\ \n",
        "\\end{cases}$ </center>\n",
        "\n",
        "Clearly discuss your observations/results for each of the three kernels.\n",
        "\n",
        "(c). Report all your observations"
      ],
      "metadata": {
        "id": "UP_gGkTIAupD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "#Generate 100 data points\n",
        "########################################\n",
        "\n",
        "\n",
        "\n",
        "#Obtian train and test splits\n",
        "#Take even samples for training and odd samples for testing\n",
        "\n",
        "\n",
        "\n",
        "def KernelRegressionFit(X_train,Y_train,kernelType,M,lamda):\n",
        "    #kernelType : {Polynomial,Gaussian,Sigmoid}\n",
        "\n",
        "\n",
        "    return w_opt\n",
        "\n",
        "def KernelRegressionPred(w_est,X_train,Y_train,X_test,Y_test,kernelType):\n",
        "    #kernelType : {Polynomial,Gaussian,Sigmoid}\n",
        "\n",
        "\n",
        "    return TrainError,TestError\n",
        "\n",
        "#Iterate through range of M values\n",
        "M_range=list(range(10))\n",
        "for M in M_range:\n",
        "    #Fit Mth order polynomial using three kernels i.e {Polynomial,Gaussian,Sigmoid}\n",
        "\n",
        "\n",
        "\n",
        "    #Predict errors on both training and testing data using estimated w\n",
        "\n",
        "\n",
        "\n",
        "    #Store them for plotting\n",
        "\n",
        "#Plot training error vs polynomial order and testing error vs polynomial order for all the three different kernels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Repeat the experiments by changing target function\n",
        "########################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "By2YRUCLwSe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Part - (4) : Understanding online training  </b>\n",
        "\n",
        "(a). Repeat 3(a) and 3(b) using stochastic gradient descent for weight update.Study the effect of step size η on convergence of the weights, and compare them to those obtained using closed form expressions in 3.  Plot the mse as a  function  of  iterations. \n",
        "\n",
        "(b). Study the effect of batch size on the speed of convergence\n",
        "\n",
        "(c). Report all your observations"
      ],
      "metadata": {
        "id": "J2PePhkvBCMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "########################################\n",
        "#Repeat 3(a) and 3(b) using stochastic gradient descent for weight update ( plot required results )\n",
        "########################################\n",
        "#Complete the function\n",
        "def OnlineTraining(X_train,Y_train,kernelType,M,Epochs,BatchSize,stepSize):\n",
        "    #kernelType : {Polynomial,Gaussian,Sigmoid}\n",
        "    #Initialize the weights\n",
        "    #Initialize the necessary variables\n",
        "    #Iterate through epochs\n",
        "        #Iterate through the batches\n",
        "            #Initialize the necessary variables\n",
        "            #Get a batch of data\n",
        "            #Iterate through the data points of obtained batch\n",
        "                #Obtain kernel representation\n",
        "                #Compute the gradient of weight's\n",
        "                #Compute the running mean of the weights gradients for the batch update\n",
        "            #Update the weights using mean gradient, consider using reasonable stepSize\n",
        "\n",
        "     return w_opt\n",
        "\n",
        "def OnlinePred(w_est,X_train,Y_train,X_test,Y_test,kernelType):\n",
        "    #kernelType : {Polynomial,Gaussian,Sigmoid}\n",
        "\n",
        "\n",
        "    return TrainError,TestError\n",
        "\n",
        "#Evalate models\n",
        "\n",
        "\n",
        "########################################\n",
        "#Study the effect of stepSize on the convergence of weights ( plot required results )\n",
        "########################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Study the effect of batchsize on the speed of convergence ( plot required results )\n",
        "########################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bmCDoFLD05nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Part - (5) : Understanding bias-variance trade-off  </b>\n",
        "\n",
        "(a). Generate L=100 datasets of noisy sinusoidal data, each having N=25  datapoints. For each dataset, fit a $M=25^{th}$ order linear regression model consisting of 24 Gaussian basis functions and one bias parameter.  Use regularized least squares, governed by the parameter λ, to estimate the parameters $\\textbf{w}$. Illustrate the concept of bias and variance using these 100 different parameter fits.\n",
        "1.   Chose three different regularization coefficeints ( low,middle and high ) \n",
        "2.   For each regularization coefficient, Obtain three plots of 100 estimated curves and their mean i.e Illustrates the concept of variance\n",
        "3. For each regularization coefficient, Obtain three plots of mean and the original function i.e Illustrates the concept of bias\n",
        "\n",
        "(b). Report all your observations\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hQ9W7st9BOVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "#Generate 100 data sets of noisy sinusoidal data\n",
        "########################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Use regularized least squares to estimate w\n",
        "########################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Illustrate the concept of Bias-Variance trade off\n",
        "########################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3vVynRcFZ5Kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Part - (6) : Understanding MAP estimate  </b>\n",
        "\n",
        "(a). Generate 100 noisy data points of a sinusoid. Fit a $20^{th}$  order  linear regression  model  with  Gaussian  basis  functions. Starting from a standard normal prior, update the statistics of the posterior density of the parameters using Bayesian sequential updates.\n",
        "\n",
        "(b). Sample a parameter vector from the posterior distribution, and obtain the curve fit for this realization. Repeat this for several times, and estimate the average of these curve fits, and compare it with the original sinusoid\n",
        "\n",
        "(c). Use the posterior distribution of the parameters to evaluate the predictive distribution of target $p(t_{0}/x_{0},X,t)$, and plot it for different number of training data points, as discussed in the class.\n",
        "\n",
        "(d). Report all your observations"
      ],
      "metadata": {
        "id": "115hZVaX9SS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "#Generate 100 data sets of noisy sinusoidal data\n",
        "########################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Updating statistics of posterior density\n",
        "########################################\n",
        "#Initialie the parameters for standard normal prior\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Iterate through the data points and update the stats of posterior density\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#You may save the stats of the posterior density while iterating through the data points for predictive distribution analysis\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Sample weight vector from posterior distribution. Estimate the curve, repeat the procedure for 100 times and get the avg fit\n",
        "########################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Predictive distribution analysis\n",
        "########################################\n",
        "#Predictive distribution analysis through sampling\n",
        "#Iterate through data points and sample weight vectors when partial data points are seen and plot the curves\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Predictive distribution analysis through variance\n",
        "#Iterate through data points and obtain necessary plots as discussed in the class\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RELdK4in9Ty6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center> <h2> <b> Real World Problems </b> </h2> </center>\n",
        "\n",
        "<b> Part - (1): Blog Feedback prediction  </b>\n",
        "This data originates from blog posts.  The raw HTML-documents of the blogposts were crawled and processed.  The regression task associated with the data is the prediction of the number of comments in the upcoming 24 hours. Inorder to simulate this situation, we choose a base-time (in the past) and select the blog posts that were published at most 72 hours before the selected basedate/time.  Then, we calculate all the features of the selected blog posts from the  information  that  was  available  at  the  basetime,  therefore  each  instance corresponds to a blog post.  The target is the number of comments that the blog post received in the next 24 hours relative to the base time. <br>\n",
        "[Link to Dataset](http://archive.ics.uci.edu/ml/datasets/BlogFeedback)\n"
      ],
      "metadata": {
        "id": "QyprvOUW9gDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PKTgbSdg98El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Part - (2): Breast Cancer Wisconsin (Prognostic) Data Set </b>\n",
        "Predict the recurrence time/disease-free time of the breast cancer patients from the first 30 features computed from a digitized image of a fine needle aspirate(FNA) of a breast mass. <br>\n",
        "[Link to Dataset](http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Prognostic%29)"
      ],
      "metadata": {
        "id": "52WwkOIg99kD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "78BbrX8v-PHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Part - (3): Multiple Output Regression </b>\n",
        "Until now, we mainly concentrated on a single target variable from the input vector. In this example, we use supply chain management data (scm1d) to predict target vector from the input vector. A brief description of multi output regression can be found in Section 3.1.5 of Bishop’s book. <br>\n",
        "[Link to Dataset](https://osdn.net/projects/sfnet_mulan/downloads/datasets/multi-target%20regression%20datasets/scm1d-train.zip) <br>\n",
        "[Dataset Description](https://arxiv.org/pdf/1211.6581.pdf)"
      ],
      "metadata": {
        "id": "FMWGo-2n-QCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#You may use scipy library to load the data set\n",
        "from scipy.io import arff  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ke_IewKbAiUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Report  </b>\n",
        "1. \n",
        "\n",
        "2. \n",
        "\n",
        "3. "
      ],
      "metadata": {
        "id": "OohdgUOoAenj"
      }
    }
  ]
}