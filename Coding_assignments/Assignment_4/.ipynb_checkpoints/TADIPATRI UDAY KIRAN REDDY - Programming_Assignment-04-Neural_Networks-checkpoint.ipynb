{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVmT47LOBy6i"
   },
   "source": [
    " <center> <h1> <b> Pattern Recognition and Machine Learning (EE5607) </b> </h1> </center>\n",
    "\n",
    "<b> Programming Assignment - 04 - Neural Networks</b>\n",
    "\n",
    "<b> Instructions </b>\n",
    "1. Plagiarism is strictly prohibited.\n",
    "2. Delayed submissions will be penalized with a scaling factor of 0.5 per day.\n",
    "3. Please DO NOT use any machine learning libraries unless and otherwise specified.\n",
    "\n",
    "\n",
    "\n",
    "<b> Assignment on Neural Networks </b> <br>\n",
    "This programming assignment gives you a chance to perform the classification task using neural networks. You will get to build a neural network from scratch and train and test it on a standard classification dataset. Further you will learn different tricks and techniques to train a neural network eficiently by observing few important issues and trying to overcome them. This includes observing the performance of the network for different activation functions and optimization algorithms. We will conclude with implementation of various regularization techniques and ResNet to overcome the problems of overfitting and vanishing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Exk1ALx4LXWA"
   },
   "outputs": [],
   "source": [
    "#All imports\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "np.random.seed(6996)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYAp-ibURvdd"
   },
   "source": [
    "<b>  Part - (1) : Train a classification network from scratch </b> \n",
    "1. Load MNIST data and create train, test splits\n",
    "2. Design a simple classification network\n",
    "\n",
    "\n",
    "> Network should consists of three Dense Layers with 512 nodes. The same architecture is used throughout the assignment to understand the effect of hyper parameters.\n",
    "\n",
    "> Use stochastic gradient descent optimization algorithm to update the parameters. You can use the learning rate suitable for the MNIST digit classification problem.\n",
    "\n",
    "> Use ReLU activation function in the first two layers. Softmax activation function should be used at the last layer to get the posterior probability of the classes.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. Train the network using MNIST training data and evaluate the performance on MNIST test data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_train := (60000, 784), Y_train := (60000, 1)\n",
      "Size of X_test := (10000, 784), Y_test := (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "#Load MNIST data.\n",
    "##################################################\n",
    "\n",
    "\n",
    "from mlxtend.data import loadlocal_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# drive.mount('/content/drive',force_remount=True)\n",
    "# root_path='/content/drive/MyDrive/PRMLAssignments/ProgrammingAssignment-01'\n",
    "root_path='/home/spcup2022/UDAY/dataset'\n",
    "train_data_path = str(root_path) + \"/\" + 'train-images.idx3-ubyte'\n",
    "train_label_path = str(root_path) + \"/\" + 'train-labels.idx1-ubyte'\n",
    "test_data_path = str(root_path) + \"/\" + 't10k-images.idx3-ubyte'\n",
    "test_label_path = str(root_path) + \"/\" + 't10k-labels.idx1-ubyte'\n",
    "X_train, Y_train = loadlocal_mnist(images_path=train_data_path, labels_path=train_label_path)\n",
    "X_test, Y_test = loadlocal_mnist(images_path=test_data_path, labels_path=test_label_path)\n",
    "\n",
    "Y_train = Y_train.reshape(-1, 1)\n",
    "Y_test = Y_test.reshape(-1, 1)\n",
    "print(\"Size of X_train := {}, Y_train := {}\".format(X_train.shape, Y_train.shape))\n",
    "print(\"Size of X_test := {}, Y_test := {}\".format(X_test.shape, Y_test.shape))\n",
    "\n",
    "def create_batches(X, Y, BatchSize):\n",
    "    N, Dx = X.shape\n",
    "    _, Dy = Y.shape\n",
    "    \n",
    "    # shuffling\n",
    "    idxs = np.arange(N)\n",
    "    np.random.shuffle(idxs)\n",
    "    X = X[idxs, :]\n",
    "    Y = Y[idxs, :]\n",
    "    \n",
    "    nBatches = int(np.ceil(N/BatchSize))\n",
    "    \n",
    "    X_batched = np.zeros((nBatches, BatchSize, Dx))\n",
    "    Y_batched = np.zeros((nBatches, BatchSize, Dy))\n",
    "    \n",
    "    for i in range(nBatches-1):\n",
    "        X_batched[i, :, :] = X[(i*BatchSize):((i+1)*BatchSize)]\n",
    "        Y_batched[i, :, :] = Y[(i*BatchSize):((i+1)*BatchSize)]\n",
    "    \n",
    "    if N%BatchSize != 0:\n",
    "        nlack = BatchSize - (N%BatchSize)\n",
    "        X_batched[nBatches-1, :, :] = np.vstack((X[((nBatches-1)*BatchSize):(nBatches*BatchSize)], X[:nlack]))\n",
    "        Y_batched[nBatches-1, :, :] = np.vstack((Y[((nBatches-1)*BatchSize):(nBatches*BatchSize)], Y[:nlack]))\n",
    "    else:\n",
    "        X_batched[nBatches-1, :, :] = X[((nBatches-1)*BatchSize):(nBatches*BatchSize)]\n",
    "        Y_batched[nBatches-1, :, :] = Y[((nBatches-1)*BatchSize):(nBatches*BatchSize)]\n",
    "\n",
    "    return X_batched, Y_batched\n",
    "\n",
    "def one_hot_encode(Y, nClasses):\n",
    "    Y_one = np.zeros((Y.shape[0], nClasses), dtype=int)\n",
    "    for i in range(nClasses):\n",
    "        ones = np.zeros(nClasses)\n",
    "        ones[i] = 1\n",
    "        Y_one[(Y == i)[:, 0], :] = ones\n",
    "    \n",
    "    return Y_one\n",
    "\n",
    "Y_train_one = one_hot_encode(Y_train, 10)\n",
    "Y_test_one = one_hot_encode(Y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "#Build the architecture\n",
    "##################################################\n",
    "\n",
    "def golrot(fan_in, fan_out):\n",
    "    std_dev = np.sqrt(2/(fan_in + fan_out))\n",
    "    W = np.random.normal(scale=std_dev, size=(fan_in, fan_out))\n",
    "    return W/(fan_in*fan_out)\n",
    "    \n",
    "def ReLu(X):\n",
    "    return np.maximum(X, np.zeros(X.shape))\n",
    "\n",
    "def grad_ReLu(X):\n",
    "    grad = np.zeros(X.shape)\n",
    "    grad[X > 0] = 1\n",
    "    return grad\n",
    "    \n",
    "# def softmax(x, temp=1.0):\n",
    "#     e = np.exp(x/temp)\n",
    "#     s = np.sum(e, axis=1).reshape(-1, 1)\n",
    "#     return e/s\n",
    "\n",
    "def softmax(x):\n",
    "    e = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    s = np.sum(e, axis=1, keepdims=True)\n",
    "    return e/s\n",
    "\n",
    "def cross_entropy(y_hat, y):\n",
    "    return -1*np.sum(y *np.log(y_hat + np.finfo(float).eps))\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    \n",
    "    Y_pred = np.argmax(y_hat, axis=1)\n",
    "    Y = np.argmax(y, axis=1)\n",
    "    \n",
    "    return np.mean(Y_pred == Y)*100\n",
    "\n",
    "class custom_DNN():\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        self.input_size = input_size\n",
    "        self.inter_size = 512\n",
    "        self.nClasses = num_classes\n",
    "        \n",
    "        self.W1 = golrot(self.input_size + 1, self.inter_size + 1)\n",
    "        self.W2 = golrot(self.inter_size + 1, self.inter_size + 1)\n",
    "        self.W3 = golrot(self.inter_size + 1, self.nClasses)\n",
    "        \n",
    "        self.Y1 = None\n",
    "        self.Y2 = None\n",
    "        self.Y3 = None\n",
    "        self.Z0 = None\n",
    "        self.Z1 = None\n",
    "        self.Z2 = None\n",
    "        self.Z3 = None\n",
    "        \n",
    "    def forward(self, X, eval=False):\n",
    "        B, I = X.shape\n",
    "        X_norm = None\n",
    "        if not eval:\n",
    "            X_norm = (X - np.mean(X, axis=1, keepdims=True))/np.sqrt(np.var(X, axis=1, keepdims=True))\n",
    "        else:\n",
    "            X_norm = X\n",
    "        # Input Layer\n",
    "        self.Z0 = np.hstack((X_norm, np.ones((B, 1))))\n",
    "        \n",
    "        # Layer 1\n",
    "        self.Y1 = self.Z0@self.W1\n",
    "        self.Z1 = ReLu(self.Y1)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.Y2 = self.Z1@self.W2\n",
    "        self.Z2 = ReLu(self.Y2)\n",
    "        \n",
    "        # Output Layer\n",
    "        self.Y3 = self.Z2@self.W3\n",
    "        logits = softmax(self.Y3)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def sgd_update(self, logits, Y, eta=1e-3, regularisation = 1e-3):\n",
    "        loss = cross_entropy(logits, Y) + 0.5*regularisation*np.sum(np.power(net.get_weights_norm(), 2))\n",
    "        \n",
    "        error = (Y - logits)\n",
    "        \n",
    "        \n",
    "        delta_3 = error\n",
    "#         print(delta_3.shape, self.W3.shape, grad_ReLu(self.Y3).shape)\n",
    "        delta_2 = np.multiply((delta_3@self.W3.T), grad_ReLu(self.Y2))\n",
    "        delta_1 = np.multiply((delta_2@self.W2.T), grad_ReLu(self.Y1))\n",
    "        \n",
    "        grad_W3 = self.Z2.T @ delta_3 + regularisation*self.W3\n",
    "        grad_W2 = self.Z1.T @ delta_2 + regularisation*self.W2\n",
    "        grad_W1 = self.Z0.T @ delta_1 + regularisation*self.W1\n",
    "        \n",
    "        self.W3 -= eta*grad_W3\n",
    "        self.W2 -= eta*grad_W2\n",
    "        self.W1 -= eta*grad_W1\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def get_weights_norm(self):\n",
    "        return [np.linalg.norm(self.W1), np.linalg.norm(self.W2), np.linalg.norm(self.W3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_batches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-dfe6043ceaff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnEpochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mX_batched\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_batched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_one\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batched\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_batched\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#         print(net.get_weights_norm())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_batches' is not defined"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "#Train the network\n",
    "##################################################\n",
    "nEpochs = 1\n",
    "step = 0\n",
    "batchSize = 32\n",
    "validFrequency = 150 # After every 50th batch\n",
    "\n",
    "net = custom_DNN(784, 10)\n",
    "\n",
    "trainLosses = []\n",
    "trainAccuracy = []\n",
    "validLosses = []\n",
    "validAccuracy = []\n",
    "weights = []\n",
    "\n",
    "while step < nEpochs:\n",
    "    X_batched, Y_batched = create_batches(X_train, Y_train_one, batchSize)\n",
    "    for i, (X, Y) in enumerate(zip(X_batched, Y_batched)):\n",
    "#         print(net.get_weights_norm())\n",
    "        logits = net.forward(X)\n",
    "        loss = net.sgd_update(logits, Y, eta=1e-1, regularisation=0)\n",
    "        \n",
    "        trainLosses.append(loss)\n",
    "        trainAccuracy.append(accuracy(logits, Y))\n",
    "        weights.append(net.get_weights_norm())\n",
    "#         print(net.Y3[0, :])\n",
    "#         print(logits[0, :])\n",
    "#         print(Y[0, :])\n",
    "        \n",
    "#         break\n",
    "        if (i)%validFrequency == 0:\n",
    "            predLogits = net.forward(X_test[0:100, :], eval=True)\n",
    "            validLosses.append(cross_entropy(predLogits, Y_test_one[0:100, :]))\n",
    "            validAccuracy.append(accuracy(predLogits, Y_test_one[0:100, :]))\n",
    "            print(\"EPOCH [{}/{}]:- Training Accuracy = {}, Training Loss = {}, Validation Accuracy = {}, Validation Loss = {}\".format(step+1, nEpochs, np.round(trainAccuracy[-1], 2), np.round(trainLosses[-1], 4), np.round(validAccuracy[-1], 2), np.round(validLosses[-1], 4)), end=\"\")\n",
    "            print(\"\\tweights\", weights[-1])\n",
    "#             print(logits[0, :])\n",
    "#             print(Y[0, :])\n",
    "    print(\"\")\n",
    "    step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3FErnHowTi4_"
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "#Test the network and write down the performance\n",
    "##################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AUF8zcrO1kd"
   },
   "source": [
    "<b> Part - (2) : Understanding activation functions </b>\n",
    "\n",
    "In this part you will learn to use different activation functions for the classification task and compare their performances.\n",
    "\n",
    "1. Train MNIST digit classification problem with different activation functions i.e. Sigmoid, Tanh, ReLU, LeakyReLU etc. You can stick to stochastic gradient descent optimization algorithm for this part\n",
    "2. Report the accuray on MNIST test data for all the experiments. Write down your observations in the report.</br>\n",
    "NOTE: You can use the classification network designed by you from scratch (or) you can build the model using pytorch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "#PyTorch implementation\n",
    "##################################################\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "torch.manual_seed(6969)\n",
    "\n",
    "class mnist_classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, num_classes, activations=[\"ReLU\"]*3, p_dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.inter_size = 512\n",
    "        self.nClasses = num_classes\n",
    "        \n",
    "        self.FC1 = nn.Linear(self.input_size, self.inter_size)\n",
    "        self.act1 = eval(\"nn.{}()\".format(activations[0]))\n",
    "        self.drop1 = nn.Dropout(p=p_dropout)\n",
    "        \n",
    "        self.FC2 = nn.Linear(self.inter_size, self.inter_size)\n",
    "        self.act2 = eval(\"nn.{}()\".format(activations[1]))\n",
    "        self.drop2 = nn.Dropout(p=p_dropout)\n",
    "        \n",
    "        self.FC3 = nn.Linear(self.inter_size, self.inter_size)\n",
    "        self.act3 = eval(\"nn.{}()\".format(activations[2]))\n",
    "        self.drop3 = nn.Dropout(p=p_dropout)\n",
    "        \n",
    "        self.FC4 = nn.Linear(self.inter_size, self.nClasses)\n",
    "        \n",
    "    \n",
    "    def forward(self, X):\n",
    "        # INPUT LAYER\n",
    "        X = X.squeeze(1)\n",
    "        \n",
    "        # Layer 1\n",
    "        X = self.FC1(X)\n",
    "        X = self.act1(X)\n",
    "        X = self.drop1(X)\n",
    "        \n",
    "        # Layer 2\n",
    "        X = self.FC2(X)\n",
    "        X = self.act2(X)\n",
    "        X = self.drop2(X)\n",
    "        \n",
    "        # Layer 3\n",
    "        X = self.FC3(X)\n",
    "        X = self.act3(X)\n",
    "        X = self.drop3(X)\n",
    "        \n",
    "        # Layer 4, OUTPUT LAYER\n",
    "        op = self.FC4(X)\n",
    "        return op\n",
    "\n",
    "def validate(net, X, Y, batchSize=32, device=\"cpu\"):\n",
    "    X_batched, Y_batched = create_batches(X, Y, batchSize)\n",
    "    acc = 0\n",
    "    los = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (X_b, Y_b) in enumerate(zip(X_batched, Y_batched)):\n",
    "            x = torch.FloatTensor(X_b).to(device)\n",
    "            y = torch.Tensor(Y_b).to(device)\n",
    "            \n",
    "            outputs = net(x)\n",
    "\n",
    "            loss = criterion(outputs, y).sum()\n",
    "            if device == \"cpu\":\n",
    "                los += loss.detach().numpy()\n",
    "            else:\n",
    "                los += loss.cpu().detach().numpy()\n",
    "\n",
    "            acc += torch.sum((torch.argmax(outputs, dim=1) == torch.argmax(y, dim=1)).float())\n",
    "    acc = 100*acc/X.shape[0]\n",
    "    if device == \"cpu\":\n",
    "        acc = acc.detach().numpy()\n",
    "    else:\n",
    "        acc = acc.cpu().detach().numpy()\n",
    "    \n",
    "    los = los/X.shape[0]\n",
    "    \n",
    "    return acc, los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(net, X_train, Y_train_one, X_test, Y_test_one, nEpochs=1, batchSize=32, validationFrequency=100, regularisation_constant=0, p_drop=0.0, verbose=True):\n",
    "    trainLosses = [[]]*nEpochs\n",
    "    trainAcces = [[]]*nEpochs\n",
    "\n",
    "    validLosses = [[]]*nEpochs\n",
    "    validAcces = [[]]*nEpochs\n",
    "    \n",
    "    if nEpochs != 1:\n",
    "        ta, va, tl, vl = trainer(net, X_train, Y_train_one, X_test, Y_test_one, nEpochs=1, batchSize=batchSize, validationFrequency=validationFrequency, regularisation_constant=regularisation_constant, p_drop=p_drop, verbose=verbose)\n",
    "        trainLosses[0] = tl\n",
    "        trainAcces[0] = ta\n",
    "        validLosses[0] = vl\n",
    "        validAcces[0] = va\n",
    "        \n",
    "        ta, va, tl, vl = trainer(net, X_train, Y_train_one, X_test, Y_test_one, nEpochs=nEpochs-1, batchSize=batchSize, validationFrequency=validationFrequency, regularisation_constant=regularisation_constant, p_drop=p_drop, verbose=verbose)\n",
    "        for e in range(nEpochs-1):\n",
    "            trainLosses[e+1] = tl[e]\n",
    "            trainAcces[e+1] = ta[e]\n",
    "            validLosses[e+1] = vl[e]\n",
    "            validAcces[e+1] = va[e]\n",
    "    else:\n",
    "        for e in range(1):\n",
    "            if verbose:\n",
    "                print(\"EPOCH:- [{}]\".format(e+1))\n",
    "            X_batched, Y_batched = create_batches(X_train, Y_train_one, batchSize)\n",
    "            for i, (X, Y) in enumerate(zip(X_batched, Y_batched)):\n",
    "                x = torch.FloatTensor(X).to(device)\n",
    "                y = torch.Tensor(Y).to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = net(x)\n",
    "                loss = criterion(outputs, y)\n",
    "\n",
    "                loss = loss.mean()\n",
    "                weights_norm = sum(p.pow(2.0).sum() if p.requires_grad==True else 0.0 for p in net.parameters())\n",
    "                loss += regularisation_constant*weights_norm\n",
    "\n",
    "                if (i+1)%validationFrequency == validationFrequency-1 or i == 0:\n",
    "                    trainAcc = torch.mean((torch.argmax(outputs, dim=1) == torch.argmax(y, dim=1)).float())*100\n",
    "        #             trainAcc = None\n",
    "                    if device == \"cpu\":\n",
    "                        trainAcc = trainAcc.detach().numpy()\n",
    "                        if verbose:\n",
    "                            print(\"Training accuracy is {}%, Training Loss is {}\".format(np.round(trainAcc, 2), np.round(loss.detach().numpy(), 5)), end=\" \")\n",
    "                    else:\n",
    "                        trainAcc = trainAcc.cpu().detach().numpy()\n",
    "                        if verbose:\n",
    "                            print(\"Training accuracy is {}%, Training Loss is {}\".format(np.round(trainAcc, 2), np.round(loss.cpu().detach().numpy(), 5)), end=\" \")\n",
    "                    trainAcces[e].append(trainAcc)\n",
    "\n",
    "\n",
    "                    validAcc, validLoss = validate(net, X_test, Y_test_one, batchSize=batchSize, device=device)\n",
    "                    validAcces[e].append(validAcc)\n",
    "                    validLosses[e].append(validLoss)\n",
    "                    if verbose:\n",
    "                        print(\"Validation accuracy is {}%, Validation Loss is {}\".format(np.round(validAcc, 2), np.round(validLoss, 5)))\n",
    "\n",
    "                loss.backward()\n",
    "                if device == \"cpu\":\n",
    "                    trainLosses[e].append(loss.detach().numpy())\n",
    "                else:\n",
    "                    trainLosses[e].append(loss.cpu().detach().numpy())\n",
    "                optimizer.step()\n",
    "            if verbose:\n",
    "                print(\"\")\n",
    "\n",
    "            for x in net.modules():\n",
    "                if isinstance(x, nn.Dropout):\n",
    "                    x.p = p_drop\n",
    "            if verbose:\n",
    "                print('Dropout updated to %f' %p_drop)\n",
    "    return trainAcces, validAcces, trainLosses, validLosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation:- ReLU, Training Accuracy:- 64.55078125, Training Loss:- 1.536078929901123, Validation Accuracy:- 63.79218673706055, Validation Loss:- 0.024156741066882384\n",
      "\n",
      "Activation:- LeakyReLU, Training Accuracy:- 65.8203125, Training Loss:- 1.4212775230407715, Validation Accuracy:- 64.5140609741211, Validation Loss:- 0.023098882388230414\n",
      "\n",
      "Activation:- Sigmoid, Training Accuracy:- 10.83984375, Training Loss:- 2.305344581604004, Validation Accuracy:- 10.867813110351562, Validation Loss:- 0.03621832705363631\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:3\"\n",
    "activations = [\"ReLU\", \"LeakyReLU\", \"Sigmoid\", \"Tanh\", \"Softplus\", \"Hardtanh\"]\n",
    "tA = []\n",
    "vA = []\n",
    "tL = []\n",
    "vL = []\n",
    "for i, act in enumerate(activations):\n",
    "    net = mnist_classifier(784, 10, activations=[act]*3)\n",
    "\n",
    "    net = net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "#     print(\"Number of trainable parameters:- \", sum(p.numel() for p in net.parameters() if p.requires_grad))\n",
    "\n",
    "\n",
    "    batchSize = 64\n",
    "    nEpochs = 2\n",
    "    regularisation_constant = 0\n",
    "    validationFrequency = 1\n",
    "    p_drop = 0.0\n",
    "\n",
    "\n",
    "    met = trainer(net, X_train[:2000], Y_train_one[:2000], X_test, Y_test_one, nEpochs=nEpochs, batchSize=batchSize, validationFrequency=validationFrequency, regularisation_constant=regularisation_constant, p_drop=p_drop, verbose=False)\n",
    "    trainAcces, validAcces, trainLosses, validLosses = met[0][0], met[1][0], met[2][0], met[3][0]\n",
    "    \n",
    "    print(\"Activation:- {}, Training Accuracy:- {}, Training Loss:- {}, Validation Accuracy:- {}, Validation Loss:- {}\\n\".format(act, np.mean(trainAcces[-1]), np.mean(trainLosses[-1]), np.mean(validAcces[-1]), np.mean(validLosses[-1])))\n",
    "    tA.append(trainAcces)\n",
    "    vA.append(validAcces)\n",
    "    tL.append(trainLosses)\n",
    "    vL.append(validLosses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Training\")\n",
    "for i in range(len(activations)):\n",
    "    plt.plot(np.array(tA[i]).flatten(), \"*-\", label=activations[i])\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.ylim(0, 110)\n",
    "plt.xlabel(\"# ITRs\")\n",
    "plt.ylabel(\"Accuracy in %\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Validation\")\n",
    "for i in range(len(activations)):\n",
    "    plt.plot(np.array(vA[i]).flatten(), \"*-\", label=activations[i])\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.ylim(0, 110)\n",
    "plt.xlabel(\"# ITRs\")\n",
    "plt.ylabel(\"Accuracy in %\")\n",
    "\n",
    "plt.suptitle(\"Various activation functions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "#Train the network with different activation functions\n",
    "##################################################\n",
    "\n",
    "\n",
    "\n",
    "batchSize = 32\n",
    "nEpochs = 1\n",
    "regularisation_constant = 0\n",
    "validationFrequency = 100\n",
    "p_drop = 0.0\n",
    "\n",
    "trainLosses = [[]]*nEpochs\n",
    "trainAcces = [[]]*nEpochs\n",
    "\n",
    "validLosses = [[]]*nEpochs\n",
    "validAcces = [[]]*nEpochs\n",
    "\n",
    "for e in range(nEpochs):\n",
    "#     print(\"EPOCH:- [{}/{}]\".format(e+1, nEpochs))\n",
    "    X_batched, Y_batched = create_batches(X_train, Y_train_one, batchSize)\n",
    "    for i, (X, Y) in enumerate(zip(X_batched, Y_batched)):\n",
    "        x = torch.FloatTensor(X)\n",
    "        y = torch.Tensor(Y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net(x)\n",
    "        loss = criterion(outputs, y)\n",
    "       \n",
    "        loss = loss.mean()\n",
    "        weights_norm = sum(p.pow(2.0).sum() if p.requires_grad==True else 0.0 for p in net.parameters())\n",
    "        loss += regularisation_constant*weights_norm\n",
    "        \n",
    "        if (i+1)%validationFrequency == validationFrequency-1 or i == 0:\n",
    "            trainAcc = torch.mean((torch.argmax(outputs, dim=1) == torch.argmax(y, dim=1)).float())*100\n",
    "            trainAcc = trainAcc.detach().numpy()\n",
    "            trainAcces[e].append(trainAcc)\n",
    "#             print(\"Training accuracy is {}%, Training Loss is {}\".format(np.round(trainAcc, 2), np.round(loss.detach().numpy(), 5)), end=\" \")\n",
    "            \n",
    "            validAcc, validLoss = validate(net, X_test, Y_test_one, batchSize=batchSize)\n",
    "            validAcces[e].append(validAcc)\n",
    "            validLosses[e].append(validLoss)\n",
    "#             print(\"Validation accuracy is {}%, Validation Loss is {}\".format(np.round(validAcc, 2), np.round(validLoss, 5)))\n",
    "            \n",
    "        loss.backward()\n",
    "        trainLosses[e].append(loss.detach().numpy())\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    for x in net.modules():\n",
    "        if isinstance(x, nn.Dropout):\n",
    "            x.p = p_drop\n",
    "#     print('Dropout updated to %f' %p_drop)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tO4gAAtCPZgO"
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "#Write down the accuracies and report your observations\n",
    "##################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENZXVRQ9QsgV"
   },
   "source": [
    "<b> Part - (3) : Understanding optimization algorithms </b>\n",
    "\n",
    "In this part you will learn to use different optimiztion algorithm apart from SGD.\n",
    "\n",
    "1. Using the best activation function from Part - (2), train the classification network using Adam optimization algorithm.\n",
    "3. Compare the accuracy of the networks trained with SGD and Adam optimization algorithms.\n",
    "4. Report your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "#Train the network using Adam optimizer\n",
    "##################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "#Test the network\n",
    "##################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5PTJVaVWUQY"
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "#Compare the accuracies an deport your observations\n",
    "##################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FXj11mJO_Dd"
   },
   "source": [
    "<b> Part - (4) : Understanding regularization methods </b>\n",
    "\n",
    "In this part of the assignment, you will learn about a few regularization techniques to reduce the overfitting problem.</br>\n",
    "Using the above built network, inculcate the following techniques to reduce the overfitting by retraining the network efficiently. Write down the accuracies for each case.\n",
    "\n",
    "1. Weight regularization\n",
    "2. Dropout with a probability of 0.5\n",
    "3. Early stopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "#Training with weight regularization\n",
    "##################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "#Training with dropout strategy\n",
    "##################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y7ZBi56TPb33"
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "#Training with early stopping criterion\n",
    "##################################################\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpmShRO0PdW3"
   },
   "source": [
    "<b> Part - (5) : Understanding vanishing gradients problem </b> </br>\n",
    "Use the best trained models in part (4) to understand the problem of vanishing gradient.\n",
    "1. Plot the norm of the gradients for various layers with/without weight regularization\n",
    "3. Repeat the same experiments with skip connections. You can choose your desired skip connections in the network by increasing the number of hidden layers.\n",
    "4. Report your observations on the differences between the plots in the above two cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "#Plots without skip connections\n",
    "##################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "#Plots with skip connections\n",
    "##################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQ9J-tcBQYg0"
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "#Observations\n",
    "##################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PRML_Assignment04.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
